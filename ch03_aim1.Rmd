---
header-includes:
  - \usepackage[para,online,flushleft]{threeparttable}
  - \usepackage{wrapfig}
  - \usepackage[singlelinecheck=false]{caption}
  - \captionsetup[subfigure]{singlelinecheck=on, labelfont=normalfont}
  - \captionsetup[figure]{labelfont=it}
output: 
  pdf_document:
    keep_tex: yes
bibliography: "/home/jflournoy/code_new/social-motives-rl-writeup/dissertation.bib"
csl: "/home/jflournoy/Rlibs/probly/bib/apa-old-doi-prefix.csl"
---

```{r setupch3, include=FALSE}
knitr::opts_chunk$set(fig.path = 'figures/', echo = F, warning = F, error = F, message = F)
library(probly)
load('rda/test-simulated-data.rda')
```


# Background

A substantial body of work on the relation between learning and attention indicates that stimulus salience, motivational relevance, and learning rate.
In laying out a programmatic approach to learning and attention, @mackintosh1975 [p. 294] concludes that the idea that learning rates are different depending on the stimulus is "formally equivalent to one of the main tenets of two-stage, attentional theories of learning, namely, the assumption that the probability of attending to a stimulus determines the probability of learning about that stimulus."
@grossberg1975 summarizes literature indicating that if two cues are paired with an unconditioned stimulus, the more salient cue will be conditioned more quickly to the extent that it may create a blocking effect against the conditioning of the less salient cue.
More recently, @denton2006 demonstrated in humans that the blocking effect is modulated by salience. 
Blocking refers to the phenomenon that when a new stimulus is conditioned in conjunction with an older stimulus that has already been associated with an outcome, the new stimulus builds up a less strong association with the outcome.
Though many models have been developed to account for this phenomenon, one explanation is that the new stimulus provides no new information for predicting the outcome (which is already perfectly predicted by the prior associated stimulus), and so it is not learned [see @shanks2010 for an overview].
In this study, using cues of colored dots and modulating salience by the density of dots, the authors showed that it is harder to block a new cue if that new cue is more salient (higher dot density) than the already conditioned cue.

Such attentional effects on learning may not be mere spandrels of our evolved learning mechanisms, but a key feature that helps learning beings learn quickly in contingent environments.

# Method

## Modeling approach

The focus of this dissertation is on describing and estimating differences in learning that is proposed to be related to differences in motives that develop during adolescence.
The goal of such estimation is to provide reliable information about the magnitude and sign of relations between observed phenomena, as well as about the uncertainty of that information.
A Bayesian approach provides a robust framework for specifying and estimating hierarchical and nonlinear models with many parameters while constraining inferences in a way that reduces the rate of making incorrect claims with confidence [@GelmanPowerCalculationsAssessing2014;@gelman_why_2012].
These are key concerns for the current work for two reasons.
First, convergence of reinforcement learning  model can be difficult to achieve, and Bayesian estimation helps by allowing one to constrain parameter values for these models in a principled and intuitive way using parametric prior distributions.
Second, in contrast to a confirmatory experiment, this dissertation seeks to explore relations between a number of theoretically related constructs; as such, the hierarchical Bayesian approach helps constrain estimates in ways that increase their efficiency [@gelman_why_2012].


## A model for reinforcement learning

In the context of this task, where the relation between the optimal response and the stimulus is constant, a simple model of the degree of learning could rely on a simple proportion of optimal responses $P_{ok}$ for each condition $k$.
The test of the hypothesis of the effect of framing would then be the difference between conditions in $P_o$. 
This simple model sacrifice precision for simplicity, and so I will be modeling the data using a reinforcement learning model with several parameters that can account for deviations from a strict Rescorla-Wagner (RW) process.
This increases the number of possible comparisons I am able to make between conditions, which may generate useful information about how motive-domain framing affects the learning process (as modeled, of course), but which also increases the complexity of patterns between conditions and parameters that must be interpreted.
It will be helpful to keep in mind that the framing can only be said to potentiate learning if, regardless of its affect on any model parameters, it does not result in higher proportions of optimal responding.

In this section, I simulate data as expected under the Rescorla-Wagner model implemented by @ahn2017 in their go-no-go model 2. 
Their original model handles binary decisions (button-press or no button-press) in response to four different cues. 
However, the form of the learning algorithm is generalizable to other binary choices in response to cues. 
In the case of the Social Probabilistic Learning Task (SPLT), participants are presented with a face (the cue), and must decide to press the left key or the right key. 
They are rewarded probabilistically such that for each cue, one or the other of the response options has an 80% chance of providing reinforcement. 
The go-no-go models used by @ahn2017 were derived from work by @guitart-masip2012. 
Their most flexible reinforcement learning model generates the probability of an action for each trial via N parameters: the learning rate, $\epsilon$, the effective size of reinforcement, $\rho$, a static bias parameter, $b$, an irreducible noise parameter, $\xi$, and a Pavlovian learning parameter, $\pi$.
In the SPLT, trial feedback does not vary by valence (responses result in reward, or no reward, but never punishment), so I use the model that does not include this Pavlovian component. 

## Reinforcement learning model for the SPLT

The model for an individual $j$'s probability of pressing the right arrow key on trial $t$ given that stimulus $s_{t}$ is presented, $P(a_{\rightarrow t} | s_{t})_{t}$, is determined by a logistic transformation of the action weight for pressing the right arrow key minus the action weight for pressing the left arrow key. 
This probability is then adjusted by a noise parameter, $0 \leq\xi_{jk}\leq1$ for each participant $j$ in condition $k$.
The noise parameter modulates the degree to which responses are non-systematic. 
When $\xi$ is 1, $P_{it} = .5$, and because each individual has a unique noise parameter for each condition, I am able to account for participants who do not learn during the task, or in a particular condition.
The full equation is:

$$
P(a_{\rightarrow t} | s_{t})_{t} = 
\text{logit}^{-1}\big(W(a_{\rightarrow t}| s_{t}) - W(a_{\leftarrow t}| s_{t})\big)\cdot(1-\xi_{jk}) + \small\frac{\xi_{jk}}{2}.
$$

The action weight is determined by a Rescorla-Wagner (RW) updating function $Q$, $W_{t}(a,s) = Q_{t}(a, s)$.
Although this may seem like a redundant step, I specify it this way to be consistent with @guitart-masip2012, and to illustrate the possibility that another parameter could impact the action weight separately from $Q$.
The function $Q$ encodes instrumental learning and is governed by the individual's learning rate for that condition, $\epsilon_{jk}$, and a scaling parameter $\rho_{jk}$ that scales the effective size of the possible rewards $r_t \in \{0, 1, 5\}$:

$$
Q_{t}(a_t, s_t) = Q_{t-1}(a_t, s_t) + \epsilon_{jk}\big(\rho_{jk}r_t - Q_{t-1}(a_t, s_t)\big)
$$

### Hierarchical Parameters

Each parameter ($\epsilon, \rho, b, \xi$) varies by condition $k \in 1:K$, and by participant $j \in 1:J$ nested in sample $m \in 1:M$. 
The structure of the hierarchical part of the model is the same for each parameter, so the following description for $\epsilon$ will serve as a description for all of the parameters.
For each individual $j$, $\beta_{\epsilon j}$ is a $K$-element row of coefficients for parameter $\epsilon$ for each condition:

$$
\beta_{\epsilon j} \sim \mathcal{N}(\delta_{\epsilon mm[j]}, \Sigma_{\epsilon})
$$
where $\delta_{\epsilon mm[j]}$ is a column of $K$ means for individual $j$'s sample $M$, as indexed in the vector $mm$, and $\Sigma_{\epsilon}$ is a $K\times K$ matrix of the covariance of individual coefficients between conditions.

Finally, across all $M$ samples, the means for each condition k are distributed such that: 

$$
\delta_{\epsilon k} \sim \mathcal{N}(\mu_{\epsilon k}, \sigma_\epsilon)
$$

where $\mu_{\epsilon k}$ is the population mean for parameter $\epsilon$ in condition $k$, and $\sigma$ is a slightly regularizing scale parameter for these means across all conditions and samples. The priors for these final parameters are:

$$
\begin{split}
\mu_\epsilon &\sim \mathcal{N}(0, 5)\\
\sigma_\epsilon &\sim \text{exponential(1)}.
\end{split}
$$

Note that in practice all parameter estimates were practically identical whether or not nesting within sample was modeled and so I describe only the model without nesting.
I maintain the description above to illustrate for the reader that it is at least practically possible to account for another level of hierarchy.
The model specification for both sets of models is available in the acompanying package.

## Simulating data

Before modeling the task data, I will confirm that this model can recover known parameters from simulated data.
Using RStan [version `r packageVersion('rstan')`; @standevelopmentteam2018], I simulate 100 data sets based on the structure of the sample data, using the same number of participants per sample (see the section on [descriptive statistics](descriptive-statistics.html), as well as precisely the same task structure. 
For this aim, it is important to be able to recover all $\mu_{\theta k}$ for $\theta \in \{\epsilon,\rho,b,\xi\}$ and $k \in \{1,2,3\}$, where 1 = Hungry/Thirsty, 2 = Popular/Unpopular, and 3 = Dating/Looking. Those parameters that account for idiosyncratic deviation from RW-expected behavior ($b,\xi$) will not vary by condition. Based on interactive simulation ([here](https://jflournoy.shinyapps.io/rw_model/)), reasonable parameter values for the control condition might be $\mu_\epsilon = -1.65$ and $\mu_\rho = -0.3$ [^1].

[^1]: Note that these are the _raw_ parameter values which are transformed such that $\epsilon^\prime \in [0,1]$ and $\rho^\prime \in [0,\infty)$. Similar to logistic regression, estimating the parameters on a scale the is not resticted improves estimation.

One early indication that a model may not be well suited to a problem is that when generating from the prior distribution, datasets are produced that either do not adequately cover the range of reasonable values, or that cover ranges that are implausible [@gabry2017]. 
The simulated data do generally cover the range of the actual data when we look just at the proportion of optimal presses over time (Figure \ref{fig:simdatcoverage}, and importantly do not show implausible behavior (all mass around extreme values like 0, 1, or .5).

```{r simdatcoverage, fig.width=5.875, fig.height=4, include=F}
print(nosmooth_multi_plot)
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/simdatcoverage-1.pdf}
\caption[Model-simulated task data]{Model-simulated task data. Each point is the mean of optimal presses on that trial, for the indicated condition, from 1 of 100 simulated data sets using just the model prior distributions. The red line is at .5, corresponding to random responding. It is apparent that the model priors allow coverage of the entire range of realistic responses.}
\label{fig:simdatcoverage}
\end{figure}

More detailed descriptions of the simulated date broken out per-simulation are available on the project's [github webpage](https://jflournoy.github.io/probly/).

The priors also generate reasonable ranges for the model parameters across these 100 simulations. 
Notably, the mass of the prior distributions is distributed across the full range of reasonable values at each level.
This results ultimately in a prior  distribution over the final probability of choosing the word on the right-hand side of the screen. These distributions cover the full range, with most mass around the null value of the final probability = .5 for choosing the left or right option.
If the data does not overwhelm the prior distribution, we can be assured that the prior is not biasing the estimate away from random responding.

FIGURES WITH DRAWS FROM PRIORS HERE

## Recovery of population parameters

After generating these simulated data sets and known parameter values, the model was then fit to these simulated data to evaluate its ability to revover the parameters. 
The mode was fit using 4 chains with 1000 warmup iterations and 500 sampling iterations per chain. 
An example from one simulation of estimated posterior densities for the final probability of a participant choosing the right-hand label shows that nearly all estimates capture the underlying generating parameter (Figure \ref{fig:pRfinalplot}).



```{r pRfinalplot, fig.width=5.875, fig.height=4, include=F}
ggplot2::theme_set(ggplot2::theme_minimal())
do.call(gridExtra::grid.arrange, pR_final_plots)
```

&nbsp;

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/pRfinalplot-1.pdf}
\caption[Correspondence of simulation generated and estimated behavior]{Correspondence of simulation generated and estimated behavior. The X-axis indexes the simulation-generated final-trial right-hand button-press probability ($P_\text{r-final}$), and the Y-axis indexes the estimated posterior median $P_\text{r-final}$. Whiskers indicate the posterior 2.5\% and 97.5\% quantiles.}
\label{fig:pRfinalplot}
\end{figure}

To examine the performance of all of the simulations, after the posteriors for each simulation estimate were sampled, the empirical cummulative density function was composed for the distribution of each parameter. 
The cumulative density for the generating value (from the simulation) was then found. 
This procedure is the same as finding the _p_-value for a test statistic using the probability density function for the relevant distribution.
If the posterior is a reasonable estimate of the generating parameter, then the generating parameter should be a random draw from that posterior. 
As such, the distribution of the _p_-value of the generating parameters (in relation to the posteriors) should be uniform. 

PLOTS SHOWING UNIFORM DISTRIBUTION HERE

## Target model and estimation of sample parameters

The model was fit in RStan [version `r packageVersion('rstan')`; @standevelopmentteam2018] using R [version `r paste(version$major,version$minor, sep = '.')`; @rcoreteam2018].


# Results

```{r}
rm(list = ls())
load('rda/fit-model-to-participant-data.rda')
```

```{r}
grid_arrange_shared_legend <- function(..., ncol = 1, widths) {
  plots <- list(...)
  g <- ggplot2::ggplotGrob(plots[[1]] + 
                             ggplot2::theme(legend.position="bottom")+
                             ggplot2::guides(linetype = ggplot2::guide_legend(override.aes=list(fill=NA)),
                                             shape = ggplot2::guide_legend(override.aes=list(alpha=.5))))$grobs
  legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
  lheight <- sum(legend$height)
  gridExtra::grid.arrange(
    do.call(gridExtra::arrangeGrob, c(lapply(plots, function(x)
      x + ggplot2::theme(legend.position="none")), list(ncol = ncol, widths = widths))),
    legend,
    nrow = 2,
    heights = grid::unit.c(ggplot2::unit(1, "npc") - lheight, lheight))
}
```

## Descriptive data

- Describe behavior across the run in each condition based on non-parameteric plots based on trial-by-trial means

```{r trialaverages, fig.width=5.875, fig.height=4, include=F}
ggplot2::theme_set(ggplot2::theme_minimal())
avg_per_sample_plot2 <- avg_per_sample_plot + 
  ggplot2::facet_wrap(~sample, nrow = 2, 
                      labeller = ggplot2::label_wrap_gen(width = 20))

grid_arrange_shared_legend(avg_per_sample_plot2, avg_across_samples, ncol = 2, widths = c(1.3,1))
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/trialaverages-1.pdf}
\caption[Average optimal behaviour over trial]{Average optimal behaviour over trial. Each point is the average across participants for that trial for the indicated condition. Best fit lines and confidence bands are estimated using generalized additive models and are for illustrative purposes only.}
\label{fig:trialaverages}
\end{figure}

- Define mean final probability of making optimal press, $P_{O}$, as the mean probability of optimal press from the last half of the run.
- This is correlated at _r_ = .96 with a simple logistic regression model and _r_ = ?? with reward learning model.

## Model estimated parameters

### Model comparisons

- Tabel of information criteria, plot of $\epsilon$ and $\xi$ parameter correlations across models.
- Basically no effect of allowing parameters to vary by sample -- individual-level parameters capture variation well by themselves.

### Validation of parameters

- Comparison to observed optimal presses $P_{O}$
  - Figure \ref{fig:parametersvbehavior}
  - The relations illustrated in this figure demonstrate that the models relation to the observed behavior is in line with expected relations based on simulation where, for any given value of $\rho$ in this range, there is a sweet-spot for $\epsilon$. 
  - This is due to learning that is too slow (low $\epsilon$, across all $\rho$), overweighting early, but "wrong" evidence (high $\rho$, lower $\epsilon$), or overweighting contradictory evidence at any point in the trial (high $\epsilon$, across all $\rho$).
  - The high noise parameter accounts for the behavior of participants who learn less during the task than would be expected by their learning rate parameters alone.
  
```{r parametersvbehavior, fig.width=5.875, fig.height=4, include=F}
allparameter_plot <- ggplot2::ggplot(lowfi_learning_df,
                ggplot2::aes(x = ep_mean,
                             y = qnorm(p_opt),
                             group = xi_mean_bin,
                             color = xi_mean)) + 
    ggplot2::geom_hline(yintercept = qnorm(.8), alpha = .5) + 
    ggplot2::geom_hline(yintercept = qnorm(.5), alpha = .5, color = 'blue') + 
    ggplot2::geom_point(alpha = .35) + 
    ggplot2::geom_smooth(method = 'gam', formula = y ~ s(x, fx = T, k = 3), 
                         alpha = .2, color = 'black', size = .5,
                         ggplot2::aes(linetype = xi_mean_bin)) + 
    ggplot2::scale_color_gradient(low = 'red', high = 'blue') + 
    ggplot2::facet_wrap(~rho_mean_bin) + 
    # ggplot2::theme(strip.text = element_blank()) + 
    ggplot2::coord_cartesian(ylim = c(qnorm(.40), qnorm(.998))) + 
    ggplot2::scale_y_continuous(breaks = qnorm(c(.5, .8, .998)), labels = c(.5, .8, .995)) +
    ggplot2::scale_x_continuous(breaks = qnorm(c(.01, .1, .4)), labels = c(.01, .1, .4)) + 
    ggplot2::labs(x = expression(paste('Median posterior ',beta[epsilon])),
                  y = 'Probability of choosing optimal lable',
                  caption = expression(paste('Facet labels are ranges for median posterior ', beta[rho])),
                  color = expression(paste('Median ', beta[xi])),
                  linetype = expression(paste('Median ', beta[xi], ' range')))
allparameter_plot
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/parametersvbehavior-1.pdf}
\caption[Run-end optimal press behavior and RW learning parameters]{Run-end optimal press behavior and RW learning parameters. Each point is a data from one participant. As the parameter $\epsilon$ increases (X-axis), probability of making an optimal choice increases to a maximum and then decreases. Facets bin increasing values of $\rho$ which also indicate increasing optimal choice. However, as expected, higher values of the noise parameter, $\xi$ (dotted line) suppress the associations between optimal choice behavior and the other two parameters.}
\label{fig:parametersvbehavior}
\end{figure}
  
- Comparison to confidence ratings
  - Confidence ratings allow another point of comparison for model parameters.
  - After each 48 trials, participants were asked to rate how confident they were, overall, that they knew which word was the best choice for each face.
  - Figure \ref{fig:confidencevparameters} shows the relations between confidence ratings at the end of all trials and the model estimated parameters.
  - The clear relations here may merely imply that the participants are aware of their behavior, and the model parameters show adequately systematic relations to that behavior such that they are also correlated with confidence ratings.
  - It is not clear whether the especially strong relation between the reward sensitivity parameter, $\rho$, is meaningful.
    - This may simply reflect that for a given learning rate, higher $\rho$ and lower $\xi$ reflect a quicker accumulation of action weight, or in other words, knowledge about the optimal response.
    - This point will be revisted later when comparing self-report scales to parameter estimates.

```{r confidencevparameters, fig.width=5.875, fig.height=4.1, include=F}
parameter_lookup <- c(
  ep = 'epsilon',
  rho = 'rho',
  xi = 'xi'
)

confidence_parameter_plot +
  ggplot2::facet_grid(condition ~ parameter, scales = 'free_x', 
                      labeller = ggplot2::labeller(
                        parameter = ggplot2::as_labeller(parameter_lookup,
                                                         default = ggplot2::label_parsed)))
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/confidencevparameters-1.pdf}
\caption[Confidence in learning and RW learning parameters]{Confidence in learning and RW learning parameters. Each point is the confidence rating for a single participant at the end of all trials (and because each participant has only one rating, a participant's value on the Y-axis is the same across all panels). A clear association between $\rho$ and confidence, and $\xi$ and confidence can be seen.}
\label{fig:confidencevparameters}
\end{figure}

## Population-level parameters

**NOTE: need to look at differences between partnered, unpartnered individuals**

```{r}
lowfi_ht_dl_t <- PairedData::yuen.t.test(
  lowfi_df$bin_5_to_end[lowfi_df$condition == 'Dating/Looking'], 
  lowfi_df$bin_5_to_end[lowfi_df$condition == 'Hungry/Thirsty'], 
  paired = T)
apa_lowfi_ht_dl_t <- paste0('*t*(', lowfi_ht_dl_t$parameter, ') = ', round(lowfi_ht_dl_t$statistic,2), 
               ', *p* = ', round(lowfi_ht_dl_t$p.value, 3), 
               ', $\\bar{D}$ = ', round(lowfi_ht_dl_t$estimate,3), 
               ' [', paste(round(lowfi_ht_dl_t$conf.int,3), collapse = ','), ']')

lowfi_ht_pu_t <- PairedData::yuen.t.test(
  lowfi_df$bin_5_to_end[lowfi_df$condition == 'Popular/Unpopular'],
  lowfi_df$bin_5_to_end[lowfi_df$condition == 'Hungry/Thirsty'], 
  paired = T)
apa_lowfi_ht_pu_t <- paste0('*t*(', lowfi_ht_pu_t$parameter, ') = ', round(lowfi_ht_pu_t$statistic,2), 
               ', *p* = ', round(lowfi_ht_pu_t$p.value, 5), 
               ', $\\bar{D}$ = ', round(lowfi_ht_pu_t$estimate,3), 
               ' [', paste(round(lowfi_ht_pu_t$conf.int,3), collapse = ','), ']')
```

Overall, learning was potentiated in the Dating/Looking, and Popular/Unpopular conditions.
Paired sample Yuen *t* tests, which are robust to non-normality [@yuen1974], reject the null of no difference between the two conditions of interest and the control condition (Dating/Looking: `r apa_lowfi_ht_dl_t`; Popular/Unpopular: `r apa_lowfi_ht_pu_t`).
This difference is also reflected in the parameter estimates (Figures \ref{fig:epsdiff}, \ref{fig:rhodiff}, \ref{fig:xidiff}).


```{r epsdiff, fig.width=3.5, fig.height=3, include=F}
epsilon_difference_plot
```

\begin{figure}
\centering
\captionsetup{width=3.25in}
\includegraphics{figures/epsdiff-1.pdf}
\caption[Condition contrasts for $\epsilon$]{Condition contrasts for $\epsilon$. Subscript number denotes condition. Condition 1 is Hungry/Thirsty, Condition 2 is Dating/Looking, Condition 3 is Popular/Unpopular. Shaded region is 95\% credible region, and tails extend to 99.5\% credible region.}
\label{fig:epsdiff}
\end{figure}

```{r rhodiff, fig.width=3.5, fig.height=3, include=F}
rho_difference_plot + 
  ggplot2::coord_cartesian(xlim = c(-1.5, .4))
```

\begin{figure}
\centering
\captionsetup{width=3.25in}
\includegraphics{figures/rhodiff-1.pdf}
\caption[Condition contrasts for $\rho$]{Condition contrasts for $\rho$. Subscript number denotes condition. Condition 1 is Hungry/Thirsty, Condition 2 is Dating/Looking, Condition 3 is Popular/Unpopular. Shaded region is 95\% credible region, and tails extend to 99.5\% credible region.}
\label{fig:rhodiff}
\end{figure}

```{r xidiff, fig.width=3.5, fig.height=3, include=F}
xi_difference_plot
```

\begin{figure}
\centering
\captionsetup{width=3.25in}
\includegraphics{figures/xidiff-1.pdf}
\caption[Condition contrasts for $\xi$]{Condition contrasts for $\xi$. Subscript number denotes condition. Condition 1 is Hungry/Thirsty, Condition 2 is Dating/Looking, Condition 3 is Popular/Unpopular. Shaded region is 95\% credible region, and tails extend to 99.5\% credible region.}
\label{fig:xidiff}
\end{figure}

PREDICTED RESPONSE BEHAVIOR BASED ON MEAN PARAMETERS GOES HERE

## Age, puberty, and learning

### Mean optimal presses

- Age and overall performance
- Puberty and overall performance

```{r poptageplot, fig.width=5.875, fig.height=4, include=F}
p_opt_age_plot
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/poptageplot-1.pdf}
\caption[poptageplot]{poptageplot}
\label{fig:poptageplot}
\end{figure}

```{r poptpdsplot, fig.width=5.875, fig.height=4, include=F}
p_opt_pds_plot
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/poptpdsplot-1.pdf}
\caption[poptpdsplot]{poptpdsplot}
\label{fig:poptpdsplot}
\end{figure}

```{r poptdiffageplot, fig.width=5.875, fig.height=4, include=F}
p_opt_diff_age_plot
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/poptdiffageplot-1.pdf}
\caption[poptdiffageplot]{poptdiffageplot}
\label{fig:poptdiffageplot}
\end{figure}

```{r poptdiffpdsplot, fig.width=5.875, fig.height=4, include=F}
p_opt_diff_pds_plot
```

\begin{figure}
\centering
\captionsetup{width=5.875in}
\includegraphics{figures/poptdiffpdsplot-1.pdf}
\caption[poptdiffpdsplot]{poptdiffpdsplot}
\label{fig:poptdiffpdsplot}
\end{figure}

- Age and parameters
- Puberty and parameters

# Discussion, Aim 1

- The model captures learning behavior well
- There is an expected and reasonable relation between model parameters and performance
- There is an expected and reasonable relation between model parameters and confidence
- Age and PDS trends indicate that in very early adolescence, participants do less well on the task
  - The cognitive demands of this task are somewhat high, and so an overall increase into middle adolescence is sensible
- Age and development trends in parameter estimates ...
- Salience matters, but this effect is stable across participants of different 

## Prior literature

- How does this inform our understanding of, first, reinforcement learning across this developmental period and, second, social-context effects on reinforcement learning?
- All work reviewed is cross-sectional, with age-group sizes of fewer than 50 particapants (and often fewer than 30).
  - Two notable excpetions: 
    - @mccormick2017 has N = 77 adolescents from age 8-17.7 years and focuses on age correlations rather than group differences.
    - @peters2017 has a longitudinal sample that contains 736 observations over 3 waves spanning ages 8-25 years.

### Performance differences across development

- Adults, or young adults, almost always perform better than adolescents or children
  - Adults do better: [@duijvenvoorde2008; @decker2015; @vandenbos2009a; @cohen2010; @christakou2013; @palminteri2016; @rosenblau2017; @peters2017; @mccormick2017].
  - In one study, adolescents performed better [@davidow2016].
    - There is nothing particularly different about this study.
      - Probabilistic reinforcement learning.
      - except that the reward feedback was paired with an image, and participants were later tested on whether they remembered previously seeing the image.
      - Notably, adults showed larger learning rate parameters than adolescents (performing worse as a result).
      - The results of this study are likely anomolous given the considerable evidence for greater adult performance on standard reinforcement learning paradigms.
  - This consistent age finding holds across operationalization of learning:
    - Probabilisitic reinforcement: [@vandenbos2009a; @cohen2010; @palminteri2016; @decker2015]
    - Deterministic reinforcement: [@duijvenvoorde2008; @peters2017]
    - Or more often described as risk-taking: Iowa gambling task [@christakou2013], balloon analogue risk task [@mccormick2017]
- Several studies did not include a measure of performance, but focus just on learning rate.
  - Learning rates are not linearly associated with performance. There's a sweet spot.
  - In studies where both learning rate and performance were measured, two studies show an association with age in the same direction [with adults performing better than younger participants; @rosenblau2017; @mccormick2017].
  - The only study in which this relation was reversed was [@davidow2016], though another study showed different relations between learning rates for positive versus negative feedback [@christakou2013].
- Instruction manipulations seem to have an outsized effect for adults.
  - Adult advantage was lessened by false instruction [@decker2015], and instruction bias was apparent in post-learning test phase.
  - Another U/P shows that a true instruction allows adults to exploit a good IGT deck without exploration (though adolescents behave in a way that indicates that if there were a false instruction, they would have an advantage).
- Some authors have suggested that adolescent reward sensitivity makes this developmental period advantageous for reward learning [@davidow2016;@mccormick2017].
  - If there is indeed a peak in reward sensitivity, it does not result in a particular advantage in standard reward learning tasks
- Overall, the data presented here represent the largest cross-sectional sample to date, and strengthen the conclusions of the largest, and only longitudinal study, that performance during reinforcement learning increases with age [@peters2017].

### Effects of social manipulations

- Very few studies examine social effects on reinfocement learning
  - Two approaches
    - Content of the stimuli of the task is social [@jones2014;@rosenblau2017]
    - Task is situated in a social context [@lockwood2016;@decker2015] and U/P
  - Those studies that have included social content in the stimuli are difficult to interpret
    - @jones2014 developed a paradigm in which, on each trial, the participant sees a picture of one of three faces.
      - The face in the picture then winks with the left or right eye, and the participant responds via button press which eye winked.
      - They then see a screen that indicates whether the participant "receives" a note from the "peer" in the picture, or whether another peer received the note.
      - The three faces vary by how often it is followed by the positive social reinforcement of "receiving" the note.
      - Reaction time to the wink cue was used as the outcome for the reinforcement learning model, and accuracy was also examined.
      - In a sample of adults (N = 37), adolescents (N = 45), and children (N = 38), they found that wink responses were more accurate for the high-reinforcement faces (but did not find a siginificant interaction with age, or report the estimated interaction parameter), and that learning rates vary by age (with younger participants having higher learning rates).
      - This study indicates that social rewards by themselves may be sufficient to induce accuracy differences for positively conditioned cues.
      - The learning rate association with age is harder to interpret given that there is no comparison, and many of the estimated learning rate parameters were 0. 
      - The task is not directly comparable to this one, but it is notable that age does not excert a strong enough differential effect on any of the tested relations to be noticed in this sample.
    - @rosenblau2017, like the present study, incorporate meaningful social information directly into their stimuli.
      - These authors develop a paradigm using preference ratings of objects in three broad domains (activities, fashion, and food).
      - They use prefernce ratings from either three adolescents or three adults as learning targets for the adolescent (N = 24) or adult (N = 21) samples respectively.
      - Specifically, on each trial, the participant guesses how much the target likes the item, and then receives feedback about the targets true score.
      - The findings from this study may be hard to interpret. It seems as if each item is seen only once by the participants, meaning that any learning that occurs requires generalization from one item to other items. In other words, learning on the task requires that there is some inherent structure to the preferences generated that can be learned.
      - The authors use the amount of prediction error, i.e., the difference between the participant's guess and the targets true preference, as a primary outcome of performance. They find that across both samples, prediction error decreases, with less prediction error for the adult group relative to the adolescent group.
      - They also formally model the learning process and find that the best fitting model incorporates reinforcement learning as well as information about the participant's own preferences.
      - Though the author's present a large number of non-significant _p_-values to support the equivalence of the adolescent and adult preference profiles, the sample size is quite underpowered to detect differences, and it is not clear what size of differences matter.
      - It is clear that more consistent ratings (whatever the assumed structure) would lead to better performance. It is also clear that the similarity of a target's and a participant's rating would influence the prediction error and estimate learning parameters. Since it is ambiguous (given the desing of this study) whether and how much these factors influence the results, any interpretation should be extremely cautious.
      - However, the finding of better adult performance is in line with above research and suggests that social versus non-social content may not make a difference with regard to the age differences in performance.
      - As mentioned above, this study also finds a smaller learning rate in adolescents.
  - Two studies have examined the effect of social context on learning. These are less directly relevant, but worth considering.
    - @lockwood2016 manupulate whether the participant is playing for herself, a friend, or no-one (that is, the points one do not accrue to anyone).
      - They find that performance and learning rate is higher in the self condition, than in the other two conditions.
    - @decker2015 and U/P use standard probabilistic reinforcement tasks, but add a cue set for which participants are given information about what the best choice is.
      - Adults are more sensitive to this information than adolescents. When the instruction is erroneous, this incurs a penatly for adult perfomance relative to adolescents.
  - Generally, social information and reinforcement results in similar patterns of learning as does non-social reinforcement.
  - Possibilities for adolescent-specific advantages come from social context literature, but not literature on social content.
    - There does seem to be preliminary support for the idea that increased option exploration may be related to performance gains in the right context [@decker2015] and U/P.
    - unfortunately this aspect is not testable in these data.

# Results from the present study

- Adds weight to the evidence for better overall performance for older participants, at least in this age range.
- We find a positive correlation between performance and learning rate, though there is some indication in this sample that there is a non-linearity such that at some point, higher learning rates impede performance. This is expected behavior of the learning model, and future work should ensure that this possibility is accounted for, or else risk over-interpretting linear correlations between learning rates and other measures.
- We also see a positive correlation between the inverse temperature parameter that governs the magnitude of rewards (versus no-reward, in this case) and performance.
  - However, non-linear relations between learning rate, temperature and performance must be carefully considered. In the mate-seeking and status conditions where performance is best, the learning rate is indeed higher than in the minimally social condition. However, the inverse temperature parameter is lower in these two conditions.
- Unsurpsisingly, the noise parameter, which estimates across the entire run the degree of random responding, is also monotonically related to performance as expected.
  - The increased performance in the two social conditions is also reflected in this parameter being, on average, lower.
- There is no effect of age on the effect of the social conditions on learning.
  - In other words, the enhancement of learning due to the increased salience of the social conditions does not vary consistently with age or pubertal status.
  - This is consistent with the tentative findings reviewed above. Given the larger sample size in this case, we can be more confident in this finding.
  - Possible explanations
    - In this age range, salience differences are at ceiling. They may arise earlier in childhood.
    - Salience differences may not be due to differences in social content per-se. For example, salience differences may be due to a prior beliefe about the extent to which one can tell from the faces themselves whether they are described one way or the other. If participants believe that there is a signal in the facial characteristics for the two social conditions, but not the mimimally social condition, they may pay more attention and learn faster.


-----

